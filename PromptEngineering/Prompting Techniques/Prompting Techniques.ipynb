{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Let's setup our experiment environment\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# openai will look for OPENAI_API_KEY in environment variables, you can also set it here, be sure to protect your key.\n",
    "# client.api_key = \"\" \n",
    "def chat(message, system_prompt=\"You are a helpful assistant.\"):\n",
    "    \"\"\"\n",
    "    Function to chat with GPT model\n",
    "\n",
    "    Args:\n",
    "        message: Message to send to the model\n",
    "        system_prompt: System prompt to set assistant behavior (default: helpful assistant)\n",
    "        \n",
    "    Returns:\n",
    "        The model's response text\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-prompting\n",
    "def meta_prompt_example():\n",
    "    # Meta-prompt instructing the model to generate a useful prompt for explaining AI concepts\n",
    "    meta_prompt = (\n",
    "        \"You are an AI prompt generator. Create a detailed, specific, and engaging prompt \"\n",
    "        \"that asks an AI assistant to explain how neural networks work in a simple way for beginners.\"\n",
    "    )\n",
    "\n",
    "    # Generate the prompt using the meta-prompt\n",
    "    generated_prompt = chat(meta_prompt, system_prompt=\"You are an AI prompt engineer.\")\n",
    "    display(Markdown(\"**Generated Prompt:**\\n\" + generated_prompt))\n",
    "\n",
    "    # Use the generated prompt to get a response from the model\n",
    "    response = chat(generated_prompt)\n",
    "    display(Markdown(\"**Model Response:**\\n\" + response))\n",
    "\n",
    "\n",
    "# Run the meta-prompting example\n",
    "meta_prompt_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain of thought \n",
    "def chain_of_thought_example():\n",
    "    # Define a chain of thought prompt that asks the model to think step by step\n",
    "    chain_of_thought_prompt = (\n",
    "        \"Let's solve this step by step:\\n\"\n",
    "        \"How many days are there in a year?\\n\\n\"\n",
    "        \"1. First, let's consider what type of year we're talking about\\n\"\n",
    "        \"2. Then, let's break down how we calculate the days\\n\"\n",
    "        \"3. Finally, let's account for any special cases\"\n",
    "    )\n",
    "\n",
    "    # Get the model's step-by-step reasoning response\n",
    "    response = chat(chain_of_thought_prompt)\n",
    "    display(Markdown(\"**Chain of Thought Response:**\\n\" + response))\n",
    "\n",
    "# Run the chain of thought example\n",
    "chain_of_thought_example()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Chain-of-Thought (CoT) Works in LLMs\n",
    "\n",
    "Local Context Matters:\n",
    "Each token is generated based on previous tokens. A structured CoT context narrows the range of likely next tokens, promoting logical progression.\n",
    "\n",
    "Reducing Entropy:\n",
    "Without CoT, the model might skip steps or generate scattered responses. CoT limits randomness, making predictions more precise.\n",
    "\n",
    "Leveraging Training Patterns:\n",
    "Models learn structured reasoning from training data. CoT activates these learned patterns, guiding responses through familiar logical sequences.\n",
    "\n",
    "Intermediate Reasoning States:\n",
    "Generating intermediate steps helps the model \"see\" its reasoning as it builds an answer, improving logical flow.\n",
    "\n",
    "Self-Consistency Checks:\n",
    "Earlier tokens set constraints, reducing the likelihood of generating inconsistent or contradictory tokens later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt injection\n",
    "# We will steal the prompt from chatgpt store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Re-prompting and Revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information Retrieval with Vector Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
