{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Let's setup our experiment environment\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# openai will look for OPENAI_API_KEY in environment variables, you can also set it here, be sure to protect your key.\n",
    "# client.api_key = \"\" \n",
    "def chat(message, system_prompt=\"You are a helpful assistant.\"):\n",
    "    \"\"\"\n",
    "    Function to chat with GPT model\n",
    "\n",
    "    Args:\n",
    "        message: Message to send to the model\n",
    "        system_prompt: System prompt to set assistant behavior (default: helpful assistant)\n",
    "        \n",
    "    Returns:\n",
    "        The model's response text\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Generated Prompt:**\n",
       "**Prompt:**\n",
       "\n",
       "\"Imagine you are a friendly AI tutor tasked with explaining the concept of neural networks to a group of beginners who have little to no background in computer science or artificial intelligence. Your goal is to make the explanation engaging, relatable, and easy to understand. \n",
       "\n",
       "1. Start by using a simple analogy, such as comparing a neural network to the way the human brain works or to a network of roads connecting different cities. \n",
       "2. Break down the key components of a neural network, including neurons, layers (input, hidden, and output), and weights, using everyday language and examples.\n",
       "3. Explain how neural networks learn from data, incorporating concepts like training, examples, and feedback, while avoiding technical jargon.\n",
       "4. Use visuals or metaphors to illustrate how information flows through the network and how it makes decisions based on the input it receives.\n",
       "5. Conclude with a brief overview of real-world applications of neural networks, such as image recognition, language translation, or recommendation systems, to show their relevance and impact.\n",
       "\n",
       "Make sure to keep the tone light and encouraging, inviting questions and curiosity from your audience!\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response:**\n",
       "Hello everyone! I’m excited to be here with you today to talk about something really fascinating: neural networks! Now, I know that sounds a bit technical, but don’t worry—I’m here to make it fun and easy to understand. Let’s dive in!\n",
       "\n",
       "### 1. The Brain and Roads Analogy\n",
       "\n",
       "Imagine your brain is like a bustling city, and the roads connecting different parts of the city are like the connections in a neural network. Just as cars travel along these roads to get from one place to another, information travels through a neural network. \n",
       "\n",
       "In our city, there are different neighborhoods (or areas of the brain) that specialize in different things—like one area for music, another for math, and another for remembering faces. Similarly, in a neural network, we have different layers that help process information in various ways.\n",
       "\n",
       "### 2. Key Components of a Neural Network\n",
       "\n",
       "Let’s break down the key parts of a neural network:\n",
       "\n",
       "- **Neurons**: Think of neurons as tiny workers in our city. Each neuron takes in information, does a little bit of work on it, and then passes it along to the next neuron. Just like workers in a factory, they each have a specific job to do.\n",
       "\n",
       "- **Layers**: There are three main types of layers in a neural network:\n",
       "  - **Input Layer**: This is where the information enters the network. Imagine it as the entrance to our city where cars (data) come in.\n",
       "  - **Hidden Layers**: These are like the neighborhoods where the real work happens. The neurons in these layers process the information, making sense of it before passing it on. There can be many hidden layers, just like a city can have many neighborhoods.\n",
       "  - **Output Layer**: This is the exit of the city, where the final decision or result comes out. It’s like the destination where the cars finally arrive after traveling through the city.\n",
       "\n",
       "- **Weights**: Now, let’s talk about weights. You can think of weights as traffic signals that control how much information each neuron should pay attention to. If a signal is green (high weight), the neuron will take that information seriously. If it’s red (low weight), it might ignore it. This helps the network decide what’s important.\n",
       "\n",
       "### 3. How Neural Networks Learn\n",
       "\n",
       "So, how do these networks learn? Imagine you’re teaching a friend to recognize different types of fruit. You show them pictures of apples, bananas, and oranges, and you tell them which is which. This is similar to how a neural network learns from data.\n",
       "\n",
       "- **Training**: During training, the network looks at lots of examples (like your fruit pictures) and tries to make guesses. At first, it might get things wrong, just like your friend might mix up an apple and an orange.\n",
       "\n",
       "- **Feedback**: After each guess, you give your friend feedback—“No, that’s not an apple; it’s an orange!” The network gets feedback too, which helps it adjust its weights (the traffic signals) to improve its guesses over time.\n",
       "\n",
       "### 4. Information Flow and Decision Making\n",
       "\n",
       "Now, let’s visualize how information flows through the network. Picture a busy intersection in our city. When a car (data) arrives, it stops at the traffic signal (weights). If the signal is green, it moves forward to the next intersection (the next neuron). Each intersection has its own set of signals, guiding the car through the city until it reaches its destination (the output layer).\n",
       "\n",
       "As the car travels, it might take different routes based on the traffic signals it encounters. Similarly, the neural network adjusts its path based on the input it receives, making decisions along the way until it arrives at a conclusion.\n",
       "\n",
       "### 5. Real-World Applications\n",
       "\n",
       "Now that we understand the basics, let’s look at some cool ways neural networks are used in the real world:\n",
       "\n",
       "- **Image Recognition**: Neural networks can help computers recognize faces in photos, just like how you can spot your friends in a crowd.\n",
       "\n",
       "- **Language Translation**: They can translate languages, making it easier for people from different countries to communicate, like having a personal translator in your pocket!\n",
       "\n",
       "- **Recommendation Systems**: Ever noticed how Netflix suggests movies you might like? That’s a neural network at work, learning from what you watch and suggesting similar content.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "And there you have it! Neural networks are like a city of tiny workers, processing information and learning from examples to make decisions. They’re used in so many amazing ways that impact our daily lives. \n",
       "\n",
       "I hope this explanation has sparked your curiosity! If you have any questions or want to know more about a specific part, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a meta-prompting example\n",
    "def meta_prompt_example():\n",
    "    # Meta-prompt instructing the model to generate a useful prompt for explaining AI concepts\n",
    "    meta_prompt = (\n",
    "        \"You are an AI prompt generator. Create a detailed, specific, and engaging prompt \"\n",
    "        \"that asks an AI assistant to explain how neural networks work in a simple way for beginners.\"\n",
    "    )\n",
    "\n",
    "    # Generate the prompt using the meta-prompt\n",
    "    generated_prompt = chat(meta_prompt, system_prompt=\"You are an AI prompt engineer.\")\n",
    "    display(Markdown(\"**Generated Prompt:**\\n\" + generated_prompt))\n",
    "\n",
    "    # Use the generated prompt to get a response from the model\n",
    "    response = chat(generated_prompt)\n",
    "    display(Markdown(\"**Model Response:**\\n\" + response))\n",
    "\n",
    "\n",
    "# Run the meta-prompting example\n",
    "meta_prompt_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Chain of Thought Response:**\n",
       "Sure! Let's go through this step by step.\n",
       "\n",
       "### Step 1: Type of Year\n",
       "There are different types of years we can consider:\n",
       "\n",
       "- **Common Year**: This is a standard year that has 365 days.\n",
       "- **Leap Year**: This occurs every 4 years (with some exceptions) and has 366 days. A leap year is defined as a year that is divisible by 4, except for end-of-century years, which must be divisible by 400 to be considered a leap year.\n",
       "\n",
       "### Step 2: Calculating Days\n",
       "- In a **common year**, there are 365 days.\n",
       "- In a **leap year**, there are 366 days.\n",
       "\n",
       "To determine whether a specific year is a leap year, we can use the following rules:\n",
       "1. If the year is divisible by 4, it is a leap year, unless:\n",
       "2. The year is divisible by 100, in which case it is not a leap year, unless:\n",
       "3. The year is also divisible by 400, in which case it is a leap year.\n",
       "\n",
       "### Step 3: Special Cases\n",
       "- For example, the year 2000 is a leap year because it is divisible by 400.\n",
       "- The year 1900 is not a leap year because, although it is divisible by 100, it is not divisible by 400.\n",
       "- The year 2020 is a leap year because it is divisible by 4 and not an end-of-century year.\n",
       "\n",
       "### Summary\n",
       "- A common year has **365 days**.\n",
       "- A leap year has **366 days**.\n",
       "- Most years are common years, but every 4 years, we have a leap year, with exceptions for years divisible by 100 but not by 400.\n",
       "\n",
       "In conclusion, the number of days in a year can be either **365 or 366**, depending on whether it is a common year or a leap year."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a chain of thought example\n",
    "def chain_of_thought_example():\n",
    "    # Define a chain of thought prompt that asks the model to think step by step\n",
    "    chain_of_thought_prompt = (\n",
    "        \"Let's solve this step by step:\\n\"\n",
    "        \"How many days are there in a year?\\n\\n\"\n",
    "        \"1. First, let's consider what type of year we're talking about\\n\"\n",
    "        \"2. Then, let's break down how we calculate the days\\n\"\n",
    "        \"3. Finally, let's account for any special cases\"\n",
    "    )\n",
    "\n",
    "    # Get the model's step-by-step reasoning response\n",
    "    response = chat(chain_of_thought_prompt)\n",
    "    display(Markdown(\"**Chain of Thought Response:**\\n\" + response))\n",
    "\n",
    "# Run the chain of thought example\n",
    "chain_of_thought_example()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Chain-of-Thought would work?\n",
    "In a large language model (LLM), text is generated one token at a time. Each new token is chosen based on patterns the model has learned from its training data and the tokens that have already been generated. When we talk about “chain of thought” (CoT) from the perspective of the LLM’s token generation process, we’re essentially considering how guiding the model to produce a structured, step-by-step reasoning process influences the model’s token choices and ultimately leads to more logical and coherent outputs.\n",
    "\n",
    "Here’s why chain of thought could work, framed in terms of the token generation algorithm of an LLM:\n",
    "\n",
    "Local Context Influences Next Tokens:\n",
    "An LLM selects each new token by examining the sequence of previously generated tokens and the statistical relationships learned during training. If the previously generated tokens outline a logical reasoning step (for example, breaking down a math problem into sub-steps), the model sees a pattern of structured reasoning rather than a jumble of facts. This structured context nudges the model to continue producing tokens that stay consistent with the reasoning process. In other words, when the model’s “running text” includes a methodical chain of thought, the most probable next token is often one that logically follows that reasoning structure.\n",
    "\n",
    "Reducing Entropy in Generation:\n",
    "Without a chain of thought, the model might jump to conclusions or skip steps. This can cause “entropy” in token prediction: the model may have many plausible next tokens that feel scattered or incomplete. When a chain of thought is present, it narrows the space of plausible next tokens to those that maintain the coherence of a step-by-step explanation. Consequently, the token generation becomes more predictable and less prone to random leaps or logical gaps.\n",
    "\n",
    "Leveraging Training Patterns:\n",
    "During training, the model sees a lot of examples where reasoning is made explicit (such as textbooks, well-structured explanations, or step-by-step solutions). Tokens in these explanations form patterns that reflect logical progression—definitions followed by inferences, premises followed by conclusions, etc. By steering the model to generate a chain of thought, we’re prompting it to tap into these learned patterns of logical progression. Each token it generates in the reasoning chain resonates with patterns of structured logic from the training corpus, making the next token more likely to be a sensible continuation of that logic.\n",
    "\n",
    "Intermediate Reasoning States:\n",
    "Consider that any complex answer can be thought of as a transformation of an initial query into a final solution. In CoT reasoning, the model generates intermediate tokens that represent partial steps or intermediate states, such as setting up a formula, listing known conditions, or analyzing a sub-problem. Having these intermediate states in the token stream means the model “sees” its own reasoning path as it’s being constructed. Each of these intermediate steps provides context to guide the next token, so the model is more likely to pick tokens that advance the reasoning rather than derail it.\n",
    "\n",
    "Self-Consistency Checks:\n",
    "When reasoning tokens accumulate, they create a context that includes conditions, constraints, and partial results. As the model continues to generate tokens, the presence of these earlier reasoning steps encourages the model to maintain consistency. For example, if an earlier token states a certain fact or step, the likelihood of generating a contradictory token later diminishes. This internal consistency further aids logical progression, because tokens that would break the chain’s logic become less likely according to the model’s learned patterns.\n",
    "\n",
    "In essence, chain of thought works in an LLM because the process of writing down intermediate reasoning steps shapes the probability distribution of the next token. By making the reasoning process explicit, the model’s token selection process is guided toward coherent, logical continuations rather than haphazard guesses. This leads to outputs that appear more “thoughtful” and logically sound.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
